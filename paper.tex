\documentclass{article}

\title{Bayes Estimators and Bayes Classifier}
\date{2018-05-08}
\author{Sean Tey, Shyam Sudhakaran, Rock Gu}


\usepackage{comment}
\usepackage{amsmath} % equation with star equation* needs the amsmath package
%\usepackage{bm} for bold symbol?
\usepackage{cancel} % cross out with arrow to 0 
\usepackage{setspace} % used to set ToC spacing
\usepackage{listings} % for code syntax highlighting
\usepackage[usenames,dvipsnames]{color}    
\usepackage[margin=1.15in]{geometry} % set margin
\usepackage{blkarray} % for matrix with annotation
\usepackage{makecell} % for matrix with annotation
\usepackage{multirow} % for tables / matrix style table?
\usepackage{hyperref} % for hyperlinks
\hypersetup{ % formate hyperlinks
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}

\hbadness=99999 % suppress underfull hbox warning

\definecolor{codeBG}{RGB}{248,248,248}

\lstset{ 
  language=R,                     % the language of the code
  basicstyle=\footnotesize\ttfamily, % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  numberstyle=\tiny\color{Blue},  % the style that is used for the line-numbers
  stepnumber=1,                   % the step between two line-numbers. If it is 1, each line
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{codeBG},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  keywordstyle=\color{NavyBlue},      % keyword style
  commentstyle=\color{Tan}\itshape,  % comment style
  stringstyle=\color{OliveGreen}    % string literal style
} 


\begin{document}

\pagenumbering{gobble}

\maketitle
\newpage

\doublespacing
\tableofcontents
\singlespacing

\newpage

\pagenumbering{arabic}

\section{Abstract}

Many popular methods of parameter estimation, such Maximum Likelihood Estimation estimate parameters by solely using information from given data. However, in many real life scenarios, you generally have prior knowledge about the parameters. Classical statistical methods disregard this knowledge in their estimators, providing a weaker estimator than if they included it. Bayesian parameter estimation attempts to incorporate prior knowledge (distributional behavior) of the parameter in question.\newline

In this paper, we will go through some simple examples of using bayesian parmeter estimation and interval estimation. Afterwards we will demonstrate an application using bayesian inference to create a classifier. We will also compare the performance of the classifier when we use MLE vs Bayes Estimators.



\section{Bayes Estimators}

\subsection{Point Estimation}

\paragraph{Bayesian Estimation of Binomial Distribution's Parameter}\mbox{}\\

Bayes Rule says:

\begin{align*} 
f_{\theta|y}(\theta|y) &= \frac{f_{\theta|y}(\theta|y)f_{\theta}(\theta)}{f_{y}(y)} \\
&= \frac{f_{y|\theta}(y|\theta)f_{\theta}(\theta)}{\int f_{\theta|y}(\theta|y)f_{\theta}(\theta) d\theta} \quad \text{(By law of total probability)}
\end{align*}

$Y \sim BIN(n,\theta)$, we want to find a bayes estimator for $\theta$, which we can get from\\ $\hat{\theta}_{Bayes} = T = E[f_{\theta|y}(\theta|y)]$ which is the expected value of the posterior. To use the bayes rule, we have to assume a prior distribution for $\theta$. For simplicity in the calculation, we will assume $\theta \sim Beta(\alpha,\beta)$ where $\alpha$ and $\beta$ are known.\newline

\noindent So now we will calculate: $f_{\theta|y}(\theta|y) = \frac{f_{\theta|y}(\theta|y)f_{\theta}(\theta)}{\int f_{\theta|y}(\theta|y)f_{\theta}(\theta) d\theta}$ by first calculating the denominator $f_{y}(y) = \int f_{\theta|y}(\theta|y)f_{\theta}(\theta) d\theta$

\begin{align*}
\int f_{\theta|y}(\theta|y)f_{\theta}(\theta) d\theta &= \int \binom{n}{y}\theta^{y}(1-\theta)^{n-y} \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1} d\theta\\
&=\binom{n}{y}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \int \theta^{y}(1-\theta)^{n-y} \theta^{\alpha-1}(1-\theta)^{\beta-1} d\theta\\
&\text{Define } \binom{n}{y}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \text{ to be $c_1$}. \\
&= c_{1} \int \theta^{\alpha + y - 1}(1-\theta)^{\beta + n-y -1}  d\theta\\
&\text{Recall that the CDF of the Beta Distribution is } = \frac{\int x^{\alpha-1} (1-x)^{\beta - 1}}{B(\alpha,\beta)} dx\\
&\text{Define }B(\alpha_0,\beta_0) = \frac{\Gamma(\alpha_0)\Gamma(\beta_0)}{\Gamma(\alpha_0+\beta_0)} 
\text{, where for our equation $\alpha_0 = \alpha + y-1$ and $\beta_0 = \beta+n-y$}\\
&= c_{1}B(\alpha_0,\beta_0) \int \frac{\theta^{\alpha + y - 1}(1-\theta)^{\beta + n-y -1}}{B(\alpha_0,\beta_0)}  d\theta \text{ (Multiply equation with }\frac{B(\alpha,\beta)}{B(\alpha,\beta)} = 1)\\ 
&= c_{1}B(\alpha_0,\beta_0) \cancelto{1}{\int \frac{\theta^{\alpha + y - 1}(1-\theta)^{\beta + n-y -1}}{B(\alpha_0,\beta_0)}  d\theta}\\
&= c_{1}B(\alpha_0,\beta_0)\\
&= \binom{n}{y}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \times \frac{\Gamma(\alpha + y-1)\Gamma(\beta+n-y)}{\Gamma(\alpha+\beta+n)}\\
&= f_{y}(y)
\end{align*}

Once we've calculated $f_{y}(y)$ we can now calculate $f_{\theta|y}(\theta|y)$.

\begin{align*}
f_{\theta|y}(\theta|y) 
&= \frac{f_{\theta|y}(\theta|y)f_{\theta}(\theta)}{\int f_{\theta|y}(\theta|y)f_{\theta}(\theta) d\theta}\\
&= \frac{\cancel{\binom{n}{y}}\theta^{y}(1-\theta)^{n-y} \cancel{\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}} \theta^{\alpha-1}(1-\theta)^{\beta-1} }  
{ \cancel{\binom{n}{y}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}} \times \frac{\Gamma(\alpha + y-1)\Gamma(\beta+n-y)}{\Gamma(\alpha+\beta+n)}}\\
&=\frac{\Gamma(\alpha+\beta+n)}{\Gamma(\alpha + y-1)\Gamma(\beta+n-y)} \theta^{\alpha+y-1} (1-\theta)^{\beta+n-y-1}\\
&=Beta(\alpha+y,\beta+n-y)
\end{align*}

So $P(\theta|Y=y) \sim Beta(\alpha+y,\beta+n-y)$ and Bayes estimator for $\theta = E[f_{\theta|y}(\theta|y)] = \frac{\alpha+y}{\alpha+\beta+n}$\newline

We can also set $\alpha=\beta=1$ to make $\theta \sim UNIF(0,1)$ because $Beta(1,1) \sim UNIF(0,1)$. This essentially says we have no prior info on $\theta$ and it also simplifies a lot of the computation.

\newpage
\subsection{Example of Point Estimation}

Let's say we flip a coin 300 times and we observe heads 90 times. We can model this as a random variable $X \sim BIN(300,p)$ where $p$ is the probability of success of getting heads. We want to find the bayes estimate for $p$, assuming the prior distribution for $p$ is $f_{p}(p) \sim Beta(1,1)$\newline

From the previous proof we know that the Bayes estimator for $p$ is $\hat{p} = E[f_{p|y}(p|x)] = \frac{\alpha+x}{\alpha+\beta+n}$, therefore $\hat{p} = \frac{1+90}{1+1+300} = \frac{91}{302}$. In comparison, the MLE of $\hat{p} = \frac{\text{number of success}}{\text{number of trials}}$ = $\frac{x}{n}$ = $\frac{90}{300}$ 


\subsection{Interval Estimation}
Consider some parameter $\theta$ of some distribution $X \sim f(\theta)$.\newline

The Bayesian confidence interval at confidence $(1-\alpha)$\% is equal to [$F_{\frac{\alpha}{2}}(x)$,$F_{1-\frac{\alpha}{2}}(x)$], where $F_{v}(x)$ is the quantile of order $v$ for the posterior distribution $f_{\theta|X}(\theta|X)$

\subsection{Example of Interval Estimation}

Let's say flipping a coin has probability $p$ to get heads. We toss the coin 200 times and we observe that the coin lands on heads 80 times. We assume the prior probability of parameter $p$ to be $P(p) \sim Beta(1,1) = UNIF(0,1)$.\newline

$X \sim BIN(200,p), y = 80, n-y = 120$ and from the previously calculated bayesian point estimator for binomial we know that $f_{p}(p|x) \sim Beta(1+y, 1+(200-y) = Beta(81,121)$.

The 95\% Bayesian confidence interval for $p$ is [$F_{0.025}(x)$,$F_{0.0975}(x)$], where $F_{v}$ is the quantile of order $v$ for the posterior distribution $f_{p}(p|x) \sim Beta(81,121)$.\newline

95\% Confidence Interval = [0.3346,0.4693]

\newpage
\section{Gaussian Naive Bayes Classifier}

\subsection{How it works}
\subsubsection{Main idea}
A Bayes classifier is an application of Bayes' Theorem: $P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}$ used to predict the likelihood of a class Y given the observed data X.\newline

For example, let's say we are trying to predict the type(class) of fruit given the features $X = x_{1},x_{2},x_{3}$ representing color, shape, and taste. The classes of fruit in our example are grape, orange, and other. We observed that $x_{1} = purple, x_{2} = round, x_{3} = sweet$. What is the probability that this fruit is of class ``grape''?\newline

We can estimate this by calculating: 
\begin{align*}
P(Y=grape|X=x_{1},x_{2},x_{3})&=\frac{P(X|Y=grape)P(Y=grape)}{P(X)}\\
&= \frac{P(x_{1},x_{2},x_{3}|grape)P(grape)}{P(X)}\\
P(Y=grape|X=purple,round,sweet) &= \frac{P(purple,round,sweet|grape)P(grape)}{P(purple,round,sweet)}
\end{align*}

\subsubsection{Naive Assumption}
There is a problem with the previous equation, because calculating $P(x_{1},x_{2},x_{3}|grape)$ can be difficult due to the joint conditional probability, especially if we have even more features. This is where the ``naive'' assumption comes in, we will \textbf{assume conditional independence} between the features $x_{1},x_{2},x_{3}$. Hence the name, Naive Bayes Classifier. By using this assumption and the chain rule for probability, this will give us:
\begin{align*}
P(Y=grape|X=purple,round,sweet)&= \frac{P(purple,round,sweet|grape)P(grape)}{P(X)}\\
&= \frac{P(purple|grape)P(round|grape)P(sweet|grape)P(grape)}{P(purple,round,sweet)}
\end{align*}

We can also manipulate $P(purple,round,sweet)$ such that it is easier to calculate using the \textbf{law of total probability}. We define the set of all classes to be k, in this fruit example, k = {grape, orange, other}. 

\begin{align*}
P(X) = \sum_{k} P(X|Y=k)P(Y=k)
\end{align*}

Giving us a new equation:

\begin{align*}
P(Y=grape|X=purple,round,sweet)&= \frac{P(purple,round,sweet|grape)P(grape)}{P(X)}\\
&= \frac{P(purple|grape)P(round|grape)P(sweet|grape)P(grape)}{\sum_{k} P(X|Y=k)P(Y=k)}\\
&= \frac{P(purple|grape)P(round|grape)P(sweet|grape)P(grape)}{P(X|grape)P(grape)+P(X|orange)P(orange)+P(X|other)P(other)}
\end{align*}

\subsubsection{Empirical estimate for discrete values}
The calculation will be much easier now that we no longer have the joint probability distributions. In fact we can get an empirical estimate of $P(x_{1} = purple|Y=grape)$ by simply counting the proportion of occurences of the color purple within the subset data enties pre-labeled as ``grape''.\newline

For example if the subset of observations $X_{i}$ with label = ``grape'' is size 100 and we see the color feature to be purple for 80 of them, then the empirical estimate would be 80/100. Storing this information allows us to ``train'' the classifier to make future predictions on new observations. 
\subsubsection{Empirical estimate for continunous values}
There is one last component to this puzzle which is how do we make empirical estimates for the conditional probability $P(x_{i}|Y=grape)$ if $x_{i}$ is a continuous value. So far the features we have: color, shape, taste are discrete features. What if we incorporate continuous features to our model such as diameter?\newline

A solution to this is to assume that $P(x_{i}|Y=grape)$ follows some distribution. In our case, we will assume that $P(x_{i}|Y=grape)$ follows a \textbf{gaussian distribution}. To calculate $P(x_{i}|Y=grape)$ we will need to estimate the mean and variance from a given feature with a given label. We can simply use sample mean and sample variance or try a bayes estimator. Note that if we had p features and k classes, we would need p$\times$k number of means and another p$\times$k variances.\newline

Once we have the estimated means and variances, we will just calculate the probability density for each continuous entry in data X. Notice that we are calculating the density function, this makes the notation $P(x_{i}|Y=grape)$ inaccurate, but we will stick to it for simplicity. This brings up another question, will the value of $P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}$ still result in a probability ranging from 0 to 1? The answer is yes, because looking at the previously expanded equation carefully:

\begin{align*}
P(Y=grape|X=purple,round,sweet) &= \frac{P(X|Y=grape)P(Y=grape)}{\sum_{k} P(X|Y=k)P(Y=k)}\\
&= \frac{P(X|grape)P(grape)}{P(X|grape)P(grape)+P(X|orange)P(orange)+P(X|other)P(other)}
\end{align*}

Notice that the numerator is part of the denominator. This essentially normalizes the equation to [0,1] because at most the highest possible value for $P(Y|X)$ is 1, which is the case where $P(X|grape)P(grape)\neq0$ and $P(X|orange)P(orange)=P(X|other)P(other)=0$. 

\newpage
\subsection{Application}

\subsubsection{Bayesian Estimation of Normal Distribution Parameters}

Assume we have a random variable $X \sim N(\mu ,\sigma ^2)$, where $\sigma^2$ is known.

prior distribution: $f_\mu(\mu) \sim N(\mu_0,\sigma_0)$

posterior distribution: $f_{\mu|x} (\mu|x ) \sim N(((\frac{n}{\sigma^2} + \frac{1}{\mu_0})^{-1}) \times ([\frac{n}{\sigma^2}\times \bar{x}] + \frac{1}{\sigma_{0}^2}\mu_0 ),(\frac{n}{\sigma^2} + \frac{1}{\mu_0})^{-1})$




\subsubsection{Description}
Here we will attempt to use the gaussian naive bayes classifier described previously to predict the movement of the Standard \& Poor's 500 stock market index. The possible prediction for classes are Y = k where k = 0,1.\newline

\noindent Y = 1, means the S\&P500 has positively increased since the previous day.\newline
Y = 0, means the S\&P500 has decreased or has not changed since the previous day.\newline

We will use stock market data/metrics such as price and volume as features or predictors. We define the total number of features to be p. The total number of observation is n where each observation is one trading day.\newline

The data matrix X will be a n$\times$p matrix and response(label) vector Y will be a n$\times$1 vector which looks like:

\[
X = 
\begin{bmatrix}
    x_{11} & x_{12} & x_{13} & \dots  & x_{1p} \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2p} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{n1} & x_{n2} & x_{n3} & \dots  & x_{np}
\end{bmatrix}
,
Y = 
\begin{bmatrix}
    y_{1} \\
    y_{2} \\
    \vdots\\
    y_{n} 
\end{bmatrix}
\]

\begin{center}
where i = 1,...,n trading days and j= 1...p stock metrics (features), $y_{n} \in {0,1}$
\end{center}

Stock market Model:

$x_i =$ ith column of X\newline

$ f_{Y|X}(Y|X) \sim BIN(1,p)$,  where $p$ is probability that the S\&P500 increases.\newline

$ f_{x_{i}|Y}(x_{i}|Y) \sim N(\mu_{i},s_i^2)$, we are assuming the variance is known and is the sample variance $s_i^2$\newline

$f_Y(Y)$ follows the empirical distribution\newline

$f_{Y|X}(Y|X) = \frac{f_{X|Y}(X|Y)}{\sum_{y \in Y}f_{X|Y}(X|Y)f_{Y}(Y)} \sim BIN(1,p)$\newline

$f_{\mu_{i}}(\mu_{i}) \sim N(0,1)$\newline

$ f_{\mu_{i}|x_{i}}(\mu_{i}|x_{i}) \sim N((\frac{n}{s_i^2} + 1)^{-1}\times[\frac{n}{s_i^2}\bar{x_{i}}],(\frac{n}{s_i^2} + 1)^{-1})$\newline


Bayes estimator:  $\hat{\mu_{i}}_{Bayes} = E[f_{\mu_{i}|x_{i}}(\mu_{i}|x_{i})] = (\frac{n}{n + s_i^2}\times \bar{x_{i}})$\newline



This shows that if we assume the prior follows the standard normal distribution, and if the sample variance is small, then the bayes estimator for the mean is very close to the mle estimator, which is just the sample average. 



\newpage

\subsubsection{R Code}

All of the code can be found at \url{https://github.com/seantey/gaussian_naive_bayes_for_sp500}.

\begin{lstlisting}[language=R]
# bayes_estimator flag toggles whether to use sample mean vs bayes estimator
gaussian_nb <- function(X,y,bayes_estimator=FALSE){
  
  gnb_check_input(X,y) # Predefined function which checks input validity
  
  # sum(y) gives total days which we observe increase in S&P500
  # length(y) gives total observations (i.e. days)
  prior_Y_one <- sum(y) / length(y)
  prior_Y_zero <- 1 - prior_Y_one 
  
  # Create subset data frames where Y = 0 or Y = 1 
  X_given_one_subset <- X[as.logical(y),]
  X_given_zero_subset <- X[as.logical(1-y),]
    
  if (bayes_estimator == FALSE){
    mew_one = sapply(X_given_one_subset,mean)
    sigma_one = sapply(X_given_one_subset,sd)
    
    mew_zero = sapply(X_given_zero_subset,mean)
    sigma_zero = sapply(X_given_zero_subset,sd)
    
  } else {
    # Some bayes estimator function output:
    n <- nrow(X)
    mew_one <- sapply(X_given_one_subset,mean) # replace mean function with something else!
    sigma_one <- sapply(X_given_one_subset,sd) # replace sd function with something else!
    mew_one <- bayesian_estimate(sigma_one,mew_one,n)

    mew_zero <- sapply(X_given_zero_subset,mean) # replace mean function with something else!
    sigma_zero <- sapply(X_given_zero_subset,sd) # replace sd function with something else!
    mew_zero <- bayesian_estimate(sigma_zero,mew_zero,n)
    
  }   
  
  # Initialize a data frame of size n x p to hold P(Xij|Y=k) for each entry of matrix X
  X_given_Y_one <- data.frame(matrix(nrow = nrow(X),ncol = ncol(X))) # Y = 1
  X_given_Y_zero <- data.frame(matrix(nrow = nrow(X),ncol = ncol(X))) # Y = 0 
  
  # For every individual entry Xij, calculate the prob(Xij|Y = 1), 
  # recall that we are fitting a gaussian distribution for this
  
  # Use appropriate mew & sigma corresponding to column p and class Y = k
  for (i in 1:nrow(X)){
    row_data = X[i,] 
    
    for (p in 1:ncol(X)){
      prob_XY <- dnorm(row_data[[p]],mew_one[p],sigma_one[p])
      X_given_Y_one[i,p] <- prob_XY
    }    
  }

  # For every individual entry Xij, calculate the prob(Xij|Y = 0)  
  for (i in 1:nrow(X)){
    row_data <- X[i,] 
    
    for (p in 1:ncol(X)){
      prob_XY <- dnorm(row_data[[p]],mew_zero[p],sigma_zero[p])
      X_given_Y_zero[i,p] <- prob_XY
      
    }    
  }
  
  # Take product of entire row from rows 1...n

  # Numerator = P(Xi1|Y=1)...P(Xip|Y=1)P(Y=1) for i = 1...n
  row_product_ones <- apply(X_given_Y_one,1,prod) 
  numerator_ones <- row_product_ones*prior_Y_one
  
  row_product_zeros <- apply(X_given_Y_zero,1,prod)
  numerator_zeros <- row_product_zeros*prior_Y_zero 

  # Add numerator for Y=1 and Y=0 to get denominator.
  # Denominator = P(Xi1|Y=1)...P(Xip|Y=1)P(Y=1) + P(Xi1|Y=0)...P(Xip|Y=1)P(Y=0)

  # Calculate posterior probability P(Y|X)  
  posteriors_ones <- numerator_ones / (numerator_ones + numerator_zeros)
  posteriors_zeros <- numerator_zeros / (numerator_ones + numerator_zeros)
  
  # Predictions
  # If P(Y=1|Xij) > P(Y=0|Xij), i.e. likelihood of class 1 is greater, 
  # then we will label the observation row i as class Y = 1
  predictions <- ifelse(posteriors_ones>posteriors_zeros,1,0)
  
  # Generate list of outputs: (1) Prior Probabilities of Y, (2) Class conditional probabilities P(Xij|Y=k), (3) Posterior probabilities, (4) Predicted Labels for each data row, (5) accuracy
  
  output_priors <- c(prior_Y_zero,prior_Y_one)

  class_conditional_DF_list <- list(X_given_Y_zero,X_given_Y_one)
  
  posteriors_DF <- data.frame(posteriors_zeros,posteriors_ones)
  
  accuracy <- sum(ifelse(predictions==y,1,0))/nrow(X)
  
  # Generate a named list for easy access to elements. 
  # E.g. output_list$priors will access prior probabilities etc
  output_list <- list(priors = output_priors, class_conditional_DF_list = class_conditional_DF_list, posteriors_DF = posteriors_DF, predictions = predictions, means_zero = mew_zero, means_one = mew_one, accuracy = accuracy,sigma_one = sigma_one,sigma_zero = sigma_zero)
  
  return(output_list)
  
}
\end{lstlisting}

\newpage

Function to check input validity. Ensures that input X is a data frame and y is a binary vectory of 1's and 0's. The function also ensures the rows(length) of X and Y are equal. E.g. If X is n$\times$p, then y must be a n$\times$1 vector.
\begin{lstlisting}[language=R]
gnb_check_input <- function(X,y){

  if(!is.data.frame(X)) {
    stop("Parameter X must be a data frame, please provide the data frame whose columns are numeric variables and rows are observations")
  }
  
  # Check if length of vector y matches number of rows in matrix X
  if(!(nrow(X)==length(y))) stop("Length of response vector y must be equal to number of rows in X")
  
  # Helper function to verify response vector input.
  binary_vector_check <- function(input_vector){
    if(!is.integer(input_vector)) stop("Please provide an integer vector")    
    is_binary <- all(input_vector <= 1) && all(input_vector >= 0)    
    if(!is_binary) stop("Please provide an integer vector of 1's and 0's")
    return(TRUE)
  }
  
  # Check if y is made of 1's and 0's
  # Where 1 = increase in price since yesterday, 0 = decrease or no increase
  binary_vector_check(y)
}

\end{lstlisting}

Recall the Bayes estimator is:  $\hat{\mu_{i}}_{Bayes} = E[f_{\mu_{i}|x_{i}}(\mu_{i}|x_{i})] = (\frac{n}{n + s_i^2}\times \bar{x_{i}})$\newline

\begin{lstlisting}[language=R]
bayesian_estimate <- function(sigma,means,n) {
  
  out <- ((sigma^2)/(sigma^2 + n))*((n/sigma^2)*means)
  return(out)
  
}

\end{lstlisting}

\newpage
\subsubsection{Findings}

For simplicity our data consists of the Adjusted Closing Price \& Volume from the stocks of the following 7 companies: Apple, Amazon, Nvidia, Facebook, Google, Alibaba, Intel. (Total of 14 features or columns)\newline

We collected data from the period 2017-04-01 to 2018-05-07, which means the number of observations is 275 trading days.\newline

After running the classifier with parameter estimation using MLE and Bayes, we see that accuracy of the classifier with MLE is 0.6072 and for Bayes it was 0.4254.

\begin{center}
\begin{tabular}{|p{3cm}||p{1cm}|p{1cm}|p{1cm}|  }
 \hline
 \multicolumn{4}{|c|}{Estimated Values when Y=1} \\
 \hline
 Estimator & APPL & FB & GOOG\\
 \hline
 MLE Price Mean & 161.34 & 167.99 & 1002.15 \\
 \hline
 Bayes Price Mean & 101.62  & 100.13 & 34.19 \\
 \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{|p{3cm}|p{1cm}|p{1cm}|p{1cm}|  }
 \hline
 \multicolumn{4}{|c|}{Sample Variance when Y=1} \\
 \hline
 Stocks & APPL & FB & GOOG\\
 \hline
 Sample Variance & 161.57 & 186.35 & 7785.57 \\
 \hline
\end{tabular}
\end{center}

\mbox{}\newline
We can see that the mle estimate for Google's stock Price is much different than the bayesian estimate. This is because when we assume the prior of standard normal and that our posterior variance is the sample variance, our bayesian estimate is very sensitive to large sample variances. Google's Price has a very high sample variance so the bayesian estimate is very low and very different from the mle estimate. Apple's Price's sample variance is not that large so we can see that it's bayesian estimate is closer to its mle than Google's Price's.

Also, the performance of the classifier is very dependent on the selection of features. So to get a higher accuracy you would need to try out different metrics and see which one gives you the best results.

\paragraph{Conclusion}\mbox{}\\

Overall in terms of accuracy, the model with MLE performed much better than the Bayesian. This is partly due to the fact that we are assuming the prior distribution follows a standard normal and the posterior variance is just the sample variance but it may also be due to our choice of features. \newline

Final Thoughts:
Bayesian Inference is a powerful methodology that includes prior information and assumptions of parameters to improve estimations. Depending on the prior distribution, Bayesian point and interval estimation can be very similar to MLE. However, if the prior distribution is poorly assumed, the estimation can be very useless. From our experience in using Bayesian estimation we would recommend to use it only if there is actual prior knowledge on the behavior of the parameters in question.\newline


\end{document}

