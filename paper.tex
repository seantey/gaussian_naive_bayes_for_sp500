\documentclass{article}

\title{Bayes Estimators and Bayes Classifier}
\date{2018-05-08}
\author{Sean Tey, Shyam Sudhakaran, Rock Gu}


\usepackage{comment}
\usepackage{amsmath} % equation with star equation* needs the amsmath package
%\usepackage{bm} for bold symbol?
\usepackage{cancel} % cross out with arrow to 0 
\usepackage{setspace} % used to set ToC spacing
\usepackage{listings} % for code syntax highlighting
\usepackage[usenames,dvipsnames]{color}    
\usepackage[margin=1.15in]{geometry} % set margin
\usepackage{blkarray} % for matrix with annotation
\usepackage{makecell} % for matrix with annotation
\usepackage{multirow} % for tables / matrix style table?
\usepackage{hyperref} % for hyperlinks
\hypersetup{ % formate hyperlinks
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}

\hbadness=99999 % suppress underfull hbox warning

\definecolor{codeBG}{RGB}{248,248,248}

\lstset{ 
  language=R,                     % the language of the code
  basicstyle=\footnotesize\ttfamily, % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  numberstyle=\tiny\color{Blue},  % the style that is used for the line-numbers
  stepnumber=1,                   % the step between two line-numbers. If it is 1, each line
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{codeBG},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  keywordstyle=\color{NavyBlue},      % keyword style
  commentstyle=\color{Tan}\itshape,  % comment style
  stringstyle=\color{OliveGreen}    % string literal style
} 


\begin{document}

\pagenumbering{gobble}

\maketitle
\newpage

\doublespacing
\tableofcontents
\singlespacing

\newpage

\pagenumbering{arabic}

\section{Abstract}

\newpage

\section{Bayesian Inference}

\newpage

\subsection{Bayes Estimators}

\subsubsection{Point Estimation}

\paragraph{Bayesian Estimation of Binomial Distribution's Parameter}\mbox{}\\

Bayes Rule says:

\begin{align*} 
P(\theta|y) &= \frac{P(y|\theta)P(\theta)}{P(y)} \\
&= \frac{P(y|\theta)P(\theta)}{\int P(y|\theta)P(\theta) d\theta} \quad \text{(By law of total probability)}
\end{align*}

$Y \sim BIN(n,\theta)$, we want to find a bayes estimator for $\theta$, which we can get from\\ $\hat{\theta}_{Bayes} = T = E[P(\theta|y)]$ which is the expected value of the posterior. To use the bayes rule, we have to assume a prior distribution for $\theta$. For simplicity in the calculation, we will assume $\theta \sim Beta(\alpha,\beta)$ where $\alpha$ and $\beta$ are known.\newline

\noindent So now we will calculate: $P(\theta|y) = \frac{P(y|\theta)P(\theta)}{\int P(y|\theta)P(\theta) d\theta}$ by first calculating the denominator $P(y) = \int P(y|\theta)P(\theta) d\theta$

\begin{align*}
\int P(y|\theta)P(\theta) d\theta &= \int \binom{n}{y}\theta^{y}(1-\theta)^{n-y} \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1} d\theta\\
&=\binom{n}{y}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \int \theta^{y}(1-\theta)^{n-y} \theta^{\alpha-1}(1-\theta)^{\beta-1} d\theta\\
&\text{Define } \binom{n}{y}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \text{ to be $c_1$}. \\
&= c_{1} \int \theta^{\alpha + y - 1}(1-\theta)^{\beta + n-y -1}  d\theta\\
&\text{Recall that the CDF of the Beta Distribution is } = \frac{\int x^{\alpha-1} (1-x)^{\beta - 1}}{B(\alpha,\beta)} dx\\
&\text{Define }B(\alpha_0,\beta_0) = \frac{\Gamma(\alpha_0)\Gamma(\beta_0)}{\Gamma(\alpha_0+\beta_0)} 
\text{, where for our equation $\alpha_0 = \alpha + y-1$ and $\beta_0 = \beta+n-y$}\\
&= c_{1}B(\alpha_0,\beta_0) \int \frac{\theta^{\alpha + y - 1}(1-\theta)^{\beta + n-y -1}}{B(\alpha_0,\beta_0)}  d\theta \text{ (Multiply equation with }\frac{B(\alpha,\beta)}{B(\alpha,\beta)} = 1)\\ 
&= c_{1}B(\alpha_0,\beta_0) \cancelto{1}{\int \frac{\theta^{\alpha + y - 1}(1-\theta)^{\beta + n-y -1}}{B(\alpha_0,\beta_0)}  d\theta}\\
&= c_{1}B(\alpha_0,\beta_0)\\
&= \binom{n}{y}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \times \frac{\Gamma(\alpha + y-1)\Gamma(\beta+n-y)}{\Gamma(\alpha+\beta+n)}\\
&= P(y)
\end{align*}

\newpage
Once we've calculated $P(y)$ we can now calculate $P(\theta|y)$.

\begin{align*}
P(\theta|y) = \frac{P(y|\theta)P(\theta)}{\int P(y|\theta)P(\theta) d\theta}
\end{align*}

\newpage

\subsubsection{Interval Estimation}

\newpage

\subsubsection{Example in R}


\newpage
\section{Gaussian Naive Bayes Classifier}

\subsection{How it works}
\subsubsection{Main idea}
A Bayes classifier is an application of Bayes' Theorem: $P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}$ used to predict the likelihood of a class Y given the observed data X.\newline

For example, let's say we are trying to predict the type(class) of fruit given the features $X = x_{1},x_{2},x_{3}$ representing color, shape, and taste. The classes of fruit in our example are grape, orange, and other. We observed that $x_{1} = purple, x_{2} = round, x_{3} = sweet$. What is the probability that this fruit is of class ``grape''?\newline

We can estimate this by calculating: 
\begin{align*}
P(Y=grape|X=x_{1},x_{2},x_{3})&=\frac{P(X|Y=grape)P(Y=grape)}{P(X)}\\
&= \frac{P(x_{1},x_{2},x_{3}|grape)P(grape)}{P(X)}\\
P(Y=grape|X=purple,round,sweet) &= \frac{P(purple,round,sweet|grape)P(grape)}{P(purple,round,sweet)}
\end{align*}

\subsubsection{Naive Assumption}
There is a problem with the previous equation, because calculating $P(x_{1},x_{2},x_{3}|grape)$ can be difficult due to the joint conditional probability, especially if we have even more features. This is where the ``naive'' assumption comes in, we will \textbf{assume conditional independence} between the features $x_{1},x_{2},x_{3}$. Hence the name, Naive Bayes Classifier. By using this assumption and the chain rule for probability, this will give us:
\begin{align*}
P(Y=grape|X=purple,round,sweet)&= \frac{P(purple,round,sweet|grape)P(grape)}{P(X)}\\
&= \frac{P(purple|grape)P(round|grape)P(sweet|grape)P(grape)}{P(purple,round,sweet)}
\end{align*}

We can also manipulate $P(purple,round,sweet)$ such that it is easier to calculate using the \textbf{law of total probability}. We define the set of all classes to be k, in this fruit example, k = {grape, orange, other}. 

\begin{align*}
P(X) = \sum_{k} P(X|Y=k)P(Y=k)
\end{align*}

Giving us a new equation:

\begin{align*}
P(Y=grape|X=purple,round,sweet)&= \frac{P(purple,round,sweet|grape)P(grape)}{P(X)}\\
&= \frac{P(purple|grape)P(round|grape)P(sweet|grape)P(grape)}{\sum_{k} P(X|Y=k)P(Y=k)}\\
&= \frac{P(purple|grape)P(round|grape)P(sweet|grape)P(grape)}{P(X|grape)P(grape)+P(X|orange)P(orange)+P(X|other)P(other)}
\end{align*}

\subsubsection{Empirical estimate for discrete values}
The calculation will be much easier now that we no longer have the joint probability distributions. In fact we can get an empirical estimate of $P(x_{1} = purple|Y=grape)$ by simply counting the proportion of occurences of the color purple within the subset data enties pre-labeled as ``grape''.\newline

For example if the subset of observations $X_{i}$ with label = ``grape'' is size 100 and we see the color feature to be purple for 80 of them, then the empirical estimate would be 80/100. Storing this information allows us to ``train'' the classifier to make future predictions on new observations. 
\subsubsection{Empirical estimate for continunous values}
There is one last component to this puzzle which is how do we make empirical estimates for the conditional probability $P(x_{i}|Y=grape)$ if $x_{i}$ is a continuous value. So far the features we have: color, shape, taste are discrete features. What if we incorporate continuous features to our model such as diameter?\newline

A solution to this is to assume that $P(x_{i}|Y=grape)$ follows some distribution. In our case, we will assume that $P(x_{i}|Y=grape)$ follows a \textbf{gaussian distribution}. To calculate $P(x_{i}|Y=grape)$ we will need to estimate the mean and variance from a given feature with a given label. We can simple use sample mean and sample variance or try a bayes estimator. Note that if we had p features and k classes, we would need p$\times$k number of means and another p$\times$k variances.\newline

Once we have the estimated means and variances, we will just calculate the probability density for each continuous entry in data X. Notice that we are calculating the density function, this makes the notation $P(x_{i}|Y=grape)$ inaccurate, but we will stick to it for simplicity. This brings up another question, will the value of $P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}$ still result in a probability ranging from 0 to 1? The answer is yes, because looking at the previously expanded equation carefully:

\begin{align*}
P(Y=grape|X=purple,round,sweet) &= \frac{P(X|Y=grape)P(Y=grape)}{\sum_{k} P(X|Y=k)P(Y=k)}\\
&= \frac{P(X|grape)P(grape)}{P(X|grape)P(grape)+P(X|orange)P(orange)+P(X|other)P(other)}
\end{align*}

Notice that the numerator is part of the denominator. This essentially normalizes the equation to [0,1] because at most the highest possible value for $P(Y|X)$ is 1, which is the case where $P(X|grape)P(grape)\neq0$ and $P(X|orange)P(orange)=P(X|other)P(other)=0$. 

\newpage
\subsection{Application Example}

\subsubsection{Description}
Here we will attempt to use the gaussian naive bayes classifier described previously to predict the movement of the Standard \& Poor's 500 stock market index. The possible prediction for classes are Y = k where k = 0,1.\newline

\noindent Y = 1, means the S\&P500 has positively increased since the previous day.\newline
Y = 0, means the S\&P500 has decreased or has not changed since the previous day.\newline

We will use stock market data/metrics such as price and volume as features or predictors. We define the total number of features to be p. The total number of observation is n where each observation is one trading day.\newline

The data matrix X will be a n$\times$p matrix and response(label) vector Y will be a n$\times$1 vector which looks like:

\[
X_{ij} = 
\begin{bmatrix}
    x_{11} & x_{12} & x_{13} & \dots  & x_{1p} \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2p} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{n1} & x_{n2} & x_{n3} & \dots  & x_{np}
\end{bmatrix}
,
Y = 
\begin{bmatrix}
    y_{1} \\
    y_{2} \\
    \vdots\\
    y_{n} 
\end{bmatrix}
\]

\begin{center}
where i = 1,...,n trading days and j= 1...p stock metrics (features), $y_{n} \in {0,1}$
\end{center}

\begin{center}
\begin{blockarray}{rcccc}
 & \thead{Metric\textsubscript{1}} & \thead{Metric\textsubscript{2}} & \thead{\dots} & \thead{\dots} & \thead{Metric\textsubscript{p}}\\
\begin{block}{>{\small}r[cccc]}
TradingDay\textsubscript{1} & $x_{11}$ & $x_{12}$ & $x_{13}$ & \dots  & $x_{1p}$ \\
TradingDay\textsubscript{2} & $x_{21}$ & $x_{22}$ & $x_{23}$ & \dots  & $x_{2p}$ \\
& $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
TradingDay\textsubscript{n} & $x_{n1}$ & $x_{n2}$ & $x_{n3}$ & \dots  & $x_{np}$ \\
\end{block}
\end{blockarray}
\end{center}

$\left[\begin{array}{r|cc}
\text{metric 1} & metric 2 & metric 3\\
x11 & x12 & x13
\end{array}\right]$

\begin{tabular}{cccc}
&&metric 1 & metric 2\\
tt&\multirow{2}{*}{$\left[\right.$}& $x11$ & $x12$\\
tt& &$x11$ & $x12$
\end{tabular}

\newpage

\subsubsection{R Code}

All of the code can be found at \url{https://github.com/seantey/gaussian_naive_bayes_for_sp500}.

\begin{lstlisting}[language=R]
# bayes_estimator flag toggles whether to use sample mean vs bayes estimator
gaussian_nb <- function(X,y,bayes_estimator=FALSE){
  
  gnb_check_input(X,y) # Predefined function which checks input validity
  
  # sum(y) gives total days which we observe increase in S&P500
  # length(y) gives total observations (i.e. days)
  prior_Y_one <- sum(y) / length(y)
  prior_Y_zero <- 1 - prior_Y_one 
  
  # Create subset data frames where Y = 0 or Y = 1 
  X_given_one_subset <- X[as.logical(y),]
  X_given_zero_subset <- X[as.logical(1-y),]
    
  if (bayes_estimator == FALSE){
    mew_one = sapply(X_given_one_subset,mean)
    sigma_one = sapply(X_given_one_subset,sd)
    
    mew_zero = sapply(X_given_zero_subset,mean)
    sigma_zero = sapply(X_given_zero_subset,sd)
    
  } else {
    # Some bayes estimator function output:
    mew_one = sapply(X_given_one_subset,mean) # replace mean function with something else!
    sigma_one = sapply(X_given_one_subset,sd) # replace sd function with something else!
    
    mew_zero = sapply(X_given_zero_subset,mean) # replace mean function with something else!
    sigma_zero = sapply(X_given_zero_subset,sd) # replace sd function with something else!
    
  }  
  
  # Initialize a data frame of size n x p to hold P(Xij|Y=k) for each entry of matrix X
  X_given_Y_one <- data.frame(matrix(nrow = nrow(X),ncol = ncol(X))) # Y = 1
  X_given_Y_zero <- data.frame(matrix(nrow = nrow(X),ncol = ncol(X))) # Y = 0 
  
  # For every individual entry Xij, calculate the prob(Xij|Y = 1), 
  # recall that we are fitting a gaussian distribution for this
  
  # Use appropriate mew & sigma corresponding to column p and class Y = k
  for (i in 1:nrow(X)){
    row_data = X[i,] 
    
    for (p in 1:ncol(X)){
      prob_XY <- dnorm(row_data[[p]],mew_one[p],sigma_one[p])
      X_given_Y_one[i,p] <- prob_XY
    }    
  }

  # For every individual entry Xij, calculate the prob(Xij|Y = 0)  
  for (i in 1:nrow(X)){
    row_data <- X[i,] 
    
    for (p in 1:ncol(X)){
      prob_XY <- dnorm(row_data[[p]],mew_zero[p],sigma_zero[p])
      X_given_Y_zero[i,p] <- prob_XY
      
    }    
  }
  
  # Take product of entire row from rows 1...n

  # Numerator = P(Xi1|Y=1)...P(Xip|Y=1)P(Y=1) for i = 1...n
  row_product_ones <- apply(X_given_Y_one,1,prod) 
  numerator_ones <- row_product_ones*prior_Y_one
  
  row_product_zeros <- apply(X_given_Y_zero,1,prod)
  numerator_zeros <- row_product_zeros*prior_Y_zero 

  # Add numerator for Y=1 and Y=0 to get denominator.
  # Denominator = P(Xi1|Y=1)...P(Xip|Y=1)P(Y=1) + P(Xi1|Y=0)...P(Xip|Y=1)P(Y=0)

  # Calculate posterior probability P(Y|X)  
  posteriors_ones <- numerator_ones / (numerator_ones + numerator_zeros)
  posteriors_zeros <- numerator_zeros / (numerator_ones + numerator_zeros)
  
  # Predictions
  # If P(Y=1|Xij) > P(Y=0|Xij), i.e. likelihood of class 1 is greater, 
  # then we will label the observation row i as class Y = 1
  predictions <- ifelse(posteriors_ones>posteriors_zeros,1,0)
  
  # Generate list of outputs: (1) Prior Probabilities of Y, (2) Class conditional probabilities P(Xij|Y=k), (3) Posterior probabilities, (4) Predicted Labels for each data row, (5) accuracy
  
  output_priors <- c(prior_Y_zero,prior_Y_one)

  class_conditional_DF_list <- list(X_given_Y_zero,X_given_Y_one)
  
  posteriors_DF <- data.frame(posteriors_zeros,posteriors_ones)
  
  accuracy <- sum(ifelse(predictions==y,1,0))/nrow(X)
  
  # Generate a named list for easy access to elements. 
  # E.g. output_list$priors will access prior probabilities etc
  output_list <- list(priors = output_priors, class_conditional_DF_list = class_conditional_DF_list,
                      posteriors_DF = posteriors_DF, predictions = predictions,
                      accuracy = accuracy)
  
  return(output_list)
  
}
\end{lstlisting}

\newpage

Function to check input validity. Ensures that input X is a data frame and y is a binary vectory of 1's and 0's. The function also ensures the rows(length) of X and Y are equal. E.g. If X is n$\times$p, then y must be a n$\times$1 vector.
\begin{lstlisting}[language=R]
gnb_check_input <- function(X,y){

  if(!is.data.frame(X)) {
    stop("Parameter X must be a data frame, please provide the data frame whose columns are numeric variables and rows are observations")
  }
  
  # Check if length of vector y matches number of rows in matrix X
  if(!(nrow(X)==length(y))) stop("Length of response vector y must be equal to number of rows in X")
  
  # Helper function to verify response vector input.
  binary_vector_check <- function(input_vector){
    if(!is.integer(input_vector)) stop("Please provide an integer vector")    
    is_binary <- all(input_vector <= 1) && all(input_vector >= 0)    
    if(!is_binary) stop("Please provide an integer vector of 1's and 0's")
    return(TRUE)
  }
  
  # Check if y is made of 1's and 0's
  # Where 1 = increase in price since yesterday, 0 = decrease or no increase
  binary_vector_check(y)
}

\end{lstlisting}

\subsubsection{Findings}

\paragraph{Results from MLE estimators}


\paragraph{Results from Bayes estimators}

\paragraph{Conclusion}

\end{document}

